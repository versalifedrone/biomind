{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pprint import pprint\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, RNN, LSTM, GRU\n",
    "from torch.nn.functional import softmax, relu\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use_cuda = torch.cuda.is_available()\n",
    "use_cuda = False\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torchtext dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the two fields: Sequence and Class\n",
    "SEQ = data.Field(sequential=True,include_lengths=True, unk_token='N')\n",
    "LABEL = data.Field(sequential=False, unk_token='1') # is_target = True ?\n",
    "\n",
    "#Load the data\n",
    "train_set, validation_set, test_set = data.TabularDataset.splits(path='./data/',\n",
    "                                                                 train='train_filtered.txt',\n",
    "                                                                 validation='val_filtered.txt',\n",
    "                                                                 test='test_filtered.txt', \n",
    "                                                                 format = 'csv',\n",
    "                                                                 fields=[('sequence', SEQ), ('label', LABEL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.681e+03, 5.664e+03, 2.603e+03, 9.560e+02, 4.280e+02, 1.480e+02,\n",
       "        7.500e+01, 4.300e+01, 9.000e+00, 4.000e+00]),\n",
       " array([  123.,  2044.,  3965.,  5886.,  7807.,  9728., 11649., 13570.,\n",
       "        15491., 17412., 19333.]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L = [len(train_set[i].sequence) for i in range(len(train_set))]\n",
    "plt.hist(L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set.fields: ['sequence', 'label']\n",
      "validation_set.fields: ['sequence', 'label']\n",
      "test_set.fields: ['sequence', 'label']\n",
      "\n",
      "size of training set 14611\n",
      "size of validation set 2084\n",
      "\n",
      "content of first training sample:\n",
      "{'sequence': ['T', 'G', 'G', 'G', 'C', 'T', 'C', 'C', 'C', 'G', 'C', 'C', 'T', 'C', 'A', 'G', 'T', 'G', 'C', 'G', 'C', 'A', 'T', 'G', 'T', 'T', 'C', 'A', 'C', 'T', 'G', 'G', 'G', 'C', 'G', 'T', 'C', 'T', 'T', 'C', 'T', 'G', 'C', 'C', 'C', 'G', 'G', 'C', 'C', 'C', 'C', 'T', 'T', 'C', 'G', 'C', 'C', 'C', 'A', 'C', 'G', 'T', 'G', 'A', 'A', 'G', 'A', 'A', 'C', 'G', 'C', 'C', 'A', 'G', 'G', 'G', 'A', 'G', 'C', 'T', 'G', 'T', 'G', 'A', 'G', 'G', 'C', 'A', 'G', 'T', 'G', 'C', 'T', 'G', 'T', 'G', 'T', 'G', 'G', 'T', 'T', 'C', 'C', 'T', 'G', 'C', 'C', 'G', 'T', 'C', 'C', 'G', 'G', 'A', 'C', 'T', 'C', 'T', 'T', 'T', 'T', 'T', 'C', 'C', 'T', 'C', 'T', 'A', 'C', 'T', 'G', 'A', 'G', 'A', 'T', 'T', 'C', 'A', 'T', 'C', 'T', 'G', 'T', 'G', 'T', 'G', 'A', 'A', 'A', 'T', 'A', 'T', 'G', 'A', 'G', 'T', 'T', 'G', 'G', 'C', 'G', 'A', 'G', 'G', 'A', 'A', 'G', 'A', 'T', 'C', 'G', 'A', 'C', 'C', 'T', 'A', 'T', 'T', 'A', 'T', 'T', 'G', 'G', 'C', 'C', 'T', 'A', 'G', 'A', 'C', 'C', 'A', 'A', 'G', 'G', 'C', 'G', 'C', 'T', 'A', 'T', 'G', 'T', 'A', 'C', 'A', 'G', 'C', 'C', 'T', 'C', 'C', 'T', 'G', 'A', 'A', 'A', 'T', 'G', 'A', 'T', 'T', 'G', 'G', 'G', 'C', 'C', 'T', 'A', 'T', 'G', 'C', 'G', 'G', 'C', 'C', 'C', 'G', 'A', 'G', 'C', 'A', 'G', 'T', 'T', 'C', 'A', 'G', 'T', 'G', 'A', 'T', 'G', 'A', 'A', 'G', 'T', 'G', 'G', 'A', 'A', 'C', 'C', 'A', 'G', 'C', 'A', 'A', 'C', 'A', 'C', 'C', 'T', 'G', 'A', 'A', 'G', 'A', 'A', 'G', 'G', 'G', 'G', 'A', 'A', 'C', 'C', 'A', 'G', 'C', 'A', 'A', 'C', 'T', 'C', 'A', 'A', 'C', 'G', 'T', 'C', 'A', 'G', 'G', 'A', 'T', 'C', 'C', 'T', 'G', 'C', 'A', 'G', 'C', 'T', 'G', 'C', 'T', 'C', 'A', 'G', 'G', 'A', 'G', 'G', 'G', 'A', 'G', 'A', 'G', 'G', 'A', 'T', 'G', 'A', 'G', 'G', 'G', 'A', 'G', 'C', 'A', 'T', 'C', 'T', 'G', 'C', 'A', 'G', 'G', 'T', 'C', 'A', 'A', 'G', 'G', 'G', 'C', 'C', 'G', 'A', 'A', 'G', 'C', 'C', 'T', 'G', 'A', 'A', 'G', 'C', 'T', 'C', 'A', 'T', 'A', 'G', 'C', 'C', 'A', 'G', 'G', 'A', 'A', 'C', 'A', 'G', 'G', 'G', 'T', 'C', 'A', 'C', 'C', 'C', 'A', 'C', 'A', 'G', 'A', 'C', 'T', 'G', 'G', 'G', 'T', 'G', 'T', 'G', 'A', 'G', 'T', 'G', 'T', 'G', 'A', 'A', 'G', 'A', 'T', 'G', 'G', 'T', 'C', 'C', 'T', 'G', 'A', 'T', 'G', 'G', 'G', 'C', 'A', 'G', 'G', 'A', 'G', 'A', 'T', 'G', 'G', 'A', 'C', 'C', 'C', 'G', 'C', 'C', 'A', 'A', 'A', 'T', 'C', 'C', 'A', 'G', 'A', 'G', 'G', 'A', 'G', 'G', 'T', 'G', 'A', 'A', 'A', 'A', 'C', 'G', 'C', 'C', 'T', 'G', 'A', 'A', 'G', 'A', 'A', 'G', 'G', 'T', 'G', 'A', 'A', 'A', 'A', 'G', 'C', 'A', 'A', 'T', 'C', 'A', 'C', 'A', 'G', 'T', 'G', 'T', 'T', 'A', 'A', 'A', 'A', 'G', 'A', 'A', 'G', 'A', 'C', 'A', 'C', 'G', 'T', 'T', 'G', 'A', 'A', 'A', 'T', 'G', 'A', 'T', 'G', 'C', 'A', 'G', 'G', 'C', 'T', 'G', 'C', 'T', 'C', 'C', 'T', 'A', 'T', 'G', 'T', 'T', 'G', 'G', 'A', 'A', 'A', 'T', 'T', 'T', 'G', 'T', 'T', 'C', 'A', 'T', 'T', 'A', 'A', 'A', 'A', 'T', 'T', 'C', 'T', 'C', 'C', 'C', 'A', 'A', 'T', 'A', 'A', 'A', 'G', 'C', 'T', 'T', 'T', 'A', 'C', 'A', 'G', 'C', 'C', 'T', 'T', 'C', 'T', 'G', 'C', 'A', 'A', 'A', 'G', 'A', 'A'], 'label': '3'}\n"
     ]
    }
   ],
   "source": [
    "print('train_set.fields:', list(train_set.fields.keys()))\n",
    "print('validation_set.fields:', list(validation_set.fields.keys()))\n",
    "print('test_set.fields:', list(test_set.fields.keys()))\n",
    "print()\n",
    "print('size of training set', len(train_set))\n",
    "print('size of validation set', len(validation_set))\n",
    "print()\n",
    "print('content of first training sample:')\n",
    "print(vars(train_set[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabularies\n",
    "SEQ.build_vocab(train_set) #NO unknows \n",
    "LABEL.build_vocab(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text fields:\n",
      " size of vocabulary: 6\n",
      " no. times the \"N\" appear in the dataset: 85\n",
      " list of vocabulary (int-to-str): ['N', '<pad>', 'A', 'T', 'G', 'C']\n",
      " list of vocabulary (str-to-int): {'N': 0, '<pad>': 1, 'A': 2, 'T': 3, 'G': 4, 'C': 5}\n",
      "Counter({'A': 12942620, 'T': 12224600, 'G': 11901485, 'C': 11648150, 'N': 85})\n",
      "\n",
      "Label fields:\n",
      " list of vocabulary (int-to-str): ['1', '0', '3', '4', '2', '5']\n",
      " list of vocabulary (str-to-int): {'1': 0, '0': 1, '3': 2, '4': 3, '2': 4, '5': 5}\n"
     ]
    }
   ],
   "source": [
    "print('Text fields:')\n",
    "print(' size of vocabulary:', len(SEQ.vocab))\n",
    "# print(\" vocabulary's embedding dimension:\", SEQ.vocab.vectors.size())\n",
    "print(' no. times the \"N\" appear in the dataset:', SEQ.vocab.freqs['N'])\n",
    "print(\" list of vocabulary (int-to-str):\", SEQ.vocab.itos)\n",
    "print(\" list of vocabulary (str-to-int):\", dict(SEQ.vocab.stoi))\n",
    "print(SEQ.vocab.freqs)\n",
    "print('\\nLabel fields:')\n",
    "#print('keys of LABEL.vocab:', list(LABEL.vocab.__dict__.keys()))\n",
    "print(\" list of vocabulary (int-to-str):\", LABEL.vocab.itos)\n",
    "print(\" list of vocabulary (str-to-int):\", dict(LABEL.vocab.stoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'3': 2874, '0': 3486, '1': 5531, '2': 814, '4': 1578, '5': 328})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABEL.vocab.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make iterator for splits\n",
    "# train_iter, val_iter = data.BucketIterator.splits((train_set, validation_set),\n",
    "#                                                      batch_size=128, \n",
    "#                                                      device=0 if use_cuda else -1,\n",
    "#                                                      sort_key=lambda x: len(x.sequence), #Sorting within the batch\n",
    "#                                                      sort_within_batch=False,\n",
    "#                                                      repeat=False)\n",
    "\n",
    "train_iter = data.BucketIterator(train_set,\n",
    "                                 batch_size=128, \n",
    "                                 device=0 if use_cuda else -1,\n",
    "                                 sort_key=lambda x: len(x.sequence), #Sorting within the batch\n",
    "                                sort = True\n",
    "                                )\n",
    "\n",
    "validation_iter = data.BucketIterator(validation_set,\n",
    "                                 batch_size=128, \n",
    "                                 device=0 if use_cuda else -1,\n",
    "                                 sort_key=lambda x: len(x.sequence), #Sorting within the batch\n",
    "                                sort = True\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking dynamical batching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(187) tensor(932)\n",
      "tensor(933) tensor(1236)\n",
      "tensor(1239) tensor(1541)\n",
      "tensor(1543) tensor(1787)\n",
      "tensor(1788) tensor(2008)\n",
      "tensor(2010) tensor(2219)\n",
      "tensor(2226) tensor(2467)\n",
      "tensor(2468) tensor(2763)\n",
      "tensor(2765) tensor(3074)\n",
      "tensor(3074) tensor(3443)\n",
      "tensor(3443) tensor(3799)\n"
     ]
    }
   ],
   "source": [
    "for i,batch in enumerate(validation_iter):\n",
    "    print(min(batch.sequence[1]),max(batch.sequence[1]))\n",
    "    if i==10 : \n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(123) tensor(511)\n",
      "tensor(514) tensor(599)\n",
      "tensor(601) tensor(677)\n",
      "tensor(677) tensor(747)\n",
      "tensor(747) tensor(816)\n",
      "tensor(816) tensor(882)\n",
      "tensor(882) tensor(930)\n",
      "tensor(930) tensor(969)\n",
      "tensor(969) tensor(1020)\n",
      "tensor(1020) tensor(1072)\n",
      "tensor(1072) tensor(1120)\n"
     ]
    }
   ],
   "source": [
    "for i,batch in enumerate(train_iter):\n",
    "    print(min(batch.sequence[1]),max(batch.sequence[1]))\n",
    "    if i==10 : \n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of batch's text: (tensor([[4, 2, 4,  ..., 5, 2, 2],\n",
      "        [4, 4, 3,  ..., 2, 5, 3],\n",
      "        [5, 3, 2,  ..., 4, 2, 4],\n",
      "        ...,\n",
      "        [2, 2, 5,  ..., 1, 1, 1],\n",
      "        [2, 2, 3,  ..., 1, 1, 1],\n",
      "        [2, 2, 1,  ..., 1, 1, 1]]), tensor([511, 511, 510, 509, 508, 504, 504, 501, 497, 494, 493, 492, 492, 491,\n",
      "        489, 488, 488, 486, 485, 484, 483, 483, 481, 481, 478, 478, 477, 477,\n",
      "        476, 474, 473, 473, 469, 469, 467, 467, 465, 464, 462, 462, 461, 460,\n",
      "        456, 453, 452, 450, 449, 447, 447, 446, 444, 443, 442, 441, 441, 439,\n",
      "        438, 437, 437, 437, 434, 434, 433, 433, 432, 430, 427, 427, 420, 420,\n",
      "        419, 417, 417, 417, 416, 414, 413, 412, 412, 411, 410, 409, 403, 402,\n",
      "        397, 396, 393, 393, 387, 378, 377, 376, 374, 374, 372, 370, 370, 368,\n",
      "        364, 360, 353, 351, 349, 348, 345, 342, 339, 335, 333, 332, 331, 328,\n",
      "        322, 318, 315, 313, 307, 307, 303, 303, 292, 289, 288, 287, 287, 247,\n",
      "        223, 123]))\n",
      "first sequence in text: tensor([4, 4, 5, 5, 3, 5, 4, 3, 5, 5, 3, 3, 5, 3, 5, 3, 3, 2, 5, 5, 4, 5, 5, 2,\n",
      "        3, 5, 3, 3, 4, 4, 5, 3, 5, 5, 3, 4, 3, 4, 4, 2, 4, 4, 5, 5, 3, 4, 5, 3,\n",
      "        4, 4, 4, 2, 2, 5, 4, 4, 4, 2, 5, 3, 3, 5, 3, 2, 2, 2, 2, 4, 4, 2, 2, 5,\n",
      "        3, 2, 3, 4, 3, 5, 3, 4, 4, 2, 2, 4, 4, 5, 3, 4, 3, 4, 4, 3, 5, 5, 2, 2,\n",
      "        4, 4, 5, 5, 2, 3, 3, 3, 3, 3, 4, 5, 3, 4, 4, 5, 3, 2, 3, 2, 2, 4, 5, 4,\n",
      "        4, 4, 4, 3, 5, 3, 5, 5, 4, 4, 2, 2, 5, 5, 2, 2, 2, 4, 4, 4, 2, 4, 5, 2,\n",
      "        5, 2, 5, 2, 4, 5, 3, 5, 3, 3, 5, 3, 3, 2, 2, 2, 2, 3, 3, 4, 2, 2, 4, 4,\n",
      "        3, 4, 3, 3, 3, 2, 5, 4, 5, 5, 5, 4, 2, 4, 2, 3, 4, 2, 2, 2, 5, 2, 4, 2,\n",
      "        2, 3, 3, 5, 3, 2, 3, 3, 3, 4, 4, 4, 5, 2, 2, 4, 2, 4, 2, 3, 4, 5, 4, 5,\n",
      "        3, 3, 2, 3, 4, 3, 2, 3, 2, 3, 2, 2, 2, 4, 5, 2, 2, 2, 4, 2, 2, 5, 2, 2,\n",
      "        5, 2, 5, 2, 4, 3, 5, 2, 5, 3, 5, 5, 3, 4, 4, 5, 4, 4, 5, 2, 2, 2, 5, 5,\n",
      "        2, 2, 2, 5, 2, 2, 2, 2, 5, 5, 2, 4, 2, 4, 3, 5, 2, 3, 5, 3, 4, 4, 4, 4,\n",
      "        2, 2, 2, 2, 4, 3, 2, 2, 5, 3, 5, 4, 4, 4, 5, 5, 5, 2, 3, 4, 4, 2, 2, 2,\n",
      "        5, 2, 4, 3, 4, 4, 5, 2, 3, 4, 4, 3, 3, 5, 4, 3, 4, 5, 5, 2, 2, 2, 3, 3,\n",
      "        5, 5, 4, 2, 2, 4, 5, 2, 2, 3, 5, 3, 3, 5, 5, 3, 4, 5, 3, 2, 2, 4, 4, 5,\n",
      "        5, 2, 3, 3, 4, 4, 2, 5, 2, 5, 2, 4, 2, 2, 3, 5, 5, 4, 2, 4, 3, 4, 2, 3,\n",
      "        4, 5, 3, 4, 3, 2, 5, 5, 5, 5, 3, 5, 2, 2, 4, 4, 2, 3, 3, 3, 2, 2, 2, 5,\n",
      "        3, 2, 2, 5, 4, 2, 2, 2, 2, 2, 3, 5, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 4,\n",
      "        3, 4, 4, 2, 3, 3, 3, 4, 3, 4, 5, 3, 5, 3, 3, 4, 3, 2, 3, 3, 3, 3, 3, 2,\n",
      "        2, 4, 3, 4, 4, 2, 3, 3, 2, 2, 2, 2, 2, 2, 5, 3, 3, 2, 5, 3, 2, 5, 5, 3,\n",
      "        3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2])\n",
      "correct label index: tensor(1)\n",
      "the actual label: 1\n"
     ]
    }
   ],
   "source": [
    "# print batch information\n",
    "batch = next(iter(train_iter))\n",
    "print(\"dimension of batch's text:\", batch.sequence)\n",
    "print(\"first sequence in text:\", batch.sequence[0][:,0])\n",
    "print(\"correct label index:\", batch.label[0])\n",
    "print(\"the actual label:\", LABEL.vocab.itos[get_numpy(batch.label[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([511, 511, 510, 509, 508, 504, 504, 501, 497, 494, 493, 492, 492, 491,\n",
      "        489, 488, 488, 486, 485, 484, 483, 483, 481, 481, 478, 478, 477, 477,\n",
      "        476, 474, 473, 473, 469, 469, 467, 467, 465, 464, 462, 462, 461, 460,\n",
      "        456, 453, 452, 450, 449, 447, 447, 446, 444, 443, 442, 441, 441, 439,\n",
      "        438, 437, 437, 437, 434, 434, 433, 433, 432, 430, 427, 427, 420, 420,\n",
      "        419, 417, 417, 417, 416, 414, 413, 412, 412, 411, 410, 409, 403, 402,\n",
      "        397, 396, 393, 393, 387, 378, 377, 376, 374, 374, 372, 370, 370, 368,\n",
      "        364, 360, 353, 351, 349, 348, 345, 342, 339, 335, 333, 332, 331, 328,\n",
      "        322, 318, 315, 313, 307, 307, 303, 303, 292, 289, 288, 287, 287, 247,\n",
      "        223, 123])\n",
      "[[  1   1   1 ...   1   1   1]\n",
      " [  1   1   1 ...   1   1   1]\n",
      " [  1   1   1 ...   1   1 -10]\n",
      " ...\n",
      " [  1   1   1 ... -10 -10 -10]\n",
      " [  1   1   1 ... -10 -10 -10]\n",
      " [  1   1   1 ... -10 -10 -10]]\n"
     ]
    }
   ],
   "source": [
    "print(batch.sequence[1])\n",
    "mask = np.array([[1 if i<max_len else -10 for i in range(max(batch.sequence[1]))] for max_len in batch.sequence[1]])\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of embeddings\n",
    "embedding_dim = SEQ.vocab.vectors.size()[1]\n",
    "num_embeddings = SEQ.vocab.vectors.size()[0]\n",
    "num_classes = len(LABEL.vocab.itos)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "        #learn a new embedding\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "\n",
    "        # use pretrained embeddings\n",
    "        \n",
    "        self.lstm = LSTM(input_size=embedding_dim,\n",
    "                         hidden_size=100,\n",
    "                         num_layers=1,\n",
    "                         bidirectional=False)\n",
    "        \n",
    "        self.l_out = Linear(in_features=200,\n",
    "                            out_features=num_classes,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = {}\n",
    "        \n",
    "        # get embeddings\n",
    "        x = self.embeddings(x)\n",
    "        \n",
    "        x = nn.utils.rnn.pack_padded_sequence(x, batch_first=True)\n",
    "        \n",
    "        # rnn returns output and last hidden state\n",
    "        x, hn = self.lstm(x, self.hidden)\n",
    "        \n",
    "        # get a fixed sized hidden representation of the entire sequence\n",
    "        out['hidden'] = x = torch.cat((torch.mean(x, dim=0), torch.max(x, dim=0)[0]), dim=1)\n",
    "        \n",
    "        # classify\n",
    "        out['out'] = softmax(self.l_out(x), dim=1)\n",
    "        return out\n",
    "\n",
    "net = Net()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "    print(\"using cuda\")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which params require grad\n",
    "{p[0]: p[1].requires_grad for p in net.named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# we filter the model's parameters such that we can remove the embedding layer, \n",
    "# which does not have requires_grad\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=0.001)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], ts)\n",
    "    # averaging the one-hot encoded vector\n",
    "    return torch.mean(correct_prediction.float())\n",
    "\n",
    "def construct_sentences(batch):\n",
    "    return [\" \".join([SEQ.vocab.itos[elm] \n",
    "                      for elm in get_numpy(batch.sequence[:,i])])\n",
    "            for i in range(batch.sequence.size()[1])]\n",
    "\n",
    "def get_labels(batch):\n",
    "    return [LABEL.vocab.itos[get_numpy(batch.label[i])] for i in range(len(batch.label))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 25000\n",
    "eval_every = 1000\n",
    "log_every = 500\n",
    "\n",
    "train_loss, train_accs = [], []\n",
    "\n",
    "net.train()\n",
    "for i, batch in enumerate(train_iter):\n",
    "        batch = batch[0]\n",
    "    if i % eval_every == 0:\n",
    "        net.eval()\n",
    "        val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "        val_meta = {'label_idx': [], 'sentences': [], 'labels': []}\n",
    "        for val_batch in val_iter:\n",
    "            output = net(val_batch.sequence)\n",
    "            # batches sizes might vary, which is why we cannot just mean the batch's loss\n",
    "            # we multiply the loss and accuracies with the batch's size,\n",
    "            # to later divide by the total size\n",
    "            val_losses += criterion(output['out'], val_batch.label) * val_batch.batch_size\n",
    "            val_accs += accuracy(output['out'], val_batch.label) * val_batch.batch_size\n",
    "            val_lengths += val_batch.batch_size\n",
    "            \n",
    "            for key, _val in output.items():\n",
    "                if key not in val_meta:\n",
    "                    val_meta[key] = []\n",
    "                val_meta[key].append(get_numpy(_val)) \n",
    "            val_meta['label_idx'].append(get_numpy(val_batch.label))\n",
    "            val_meta['sentences'].append(construct_sentences(val_batch))\n",
    "            val_meta['labels'].append(get_labels(val_batch))\n",
    "        \n",
    "        for key, _val in val_meta.items():\n",
    "            val_meta[key] = np.concatenate(_val)\n",
    "        \n",
    "        # divide by the total accumulated batch sizes\n",
    "        val_losses /= val_lengths\n",
    "        val_accs /= val_lengths\n",
    "        \n",
    "        print(\"### EVAL loss: {:.2f} accs: {:.2f}\".format(get_numpy(val_losses),\n",
    "                                                          get_numpy(val_accs)))\n",
    "        net.eval()\n",
    "\n",
    "        \n",
    "    output = net(batch.text)\n",
    "    batch_loss = criterion(output['out'], batch.label)\n",
    "    \n",
    "    train_loss.append(get_numpy(batch_loss))\n",
    "    train_accs.append(get_numpy(accuracy(output['out'], batch.label)))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % log_every == 0:        \n",
    "        print(\"train, it: {} loss: {:.2f} accs: {:.2f}\".format(i, \n",
    "                                                               np.mean(train_loss), \n",
    "                                                               np.mean(train_accs)))\n",
    "        # reset\n",
    "        train_loss, train_accs = [], []\n",
    "        \n",
    "    \n",
    "    if max_iter < i:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
