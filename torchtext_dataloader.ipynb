{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pprint import pprint\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, RNN, LSTM, GRU\n",
    "from torch.nn.functional import softmax, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use_cuda = torch.cuda.is_available()\n",
    "use_cuda = False\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the two fields: Sequence and Class\n",
    "SEQ = data.Field(sequential=True)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "#Load the data\n",
    "train_set, validation_set, test_set = data.TabularDataset.splits(path='./data/',\n",
    "                                                                 train='train_filtered.txt',\n",
    "                                                                 validation='val_filtered.txt',\n",
    "                                                                 test='test_filtered.txt', \n",
    "                                                                 format = 'csv',\n",
    "                                                                 fields=[('sequence', SEQ), ('label', LABEL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set.fields: ['sequence', 'label']\n",
      "validation_set.fields: ['sequence', 'label']\n",
      "test_set.fields: ['sequence', 'label']\n",
      "\n",
      "size of training set 14611\n",
      "size of validation set 2084\n",
      "\n",
      "content of first training sample:\n",
      "{'label': '3',\n",
      " 'sequence': ['T',\n",
      "              'G',\n",
      "              'G',\n",
      "              'G',\n",
      "              'C',\n",
      "              'T',\n",
      "              'C',\n",
      "              'C',\n",
      "              'C',\n",
      "              'G',\n",
      "              'C',\n",
      "              'C',\n",
      "              'T',\n",
      "              'C',\n",
      "              'A',\n",
      "              'G',\n",
      "              'T',\n",
      "              'G',\n",
      "              'C',\n",
      "              'G',\n",
      "              'C',\n",
      "              'A',\n",
      "              'T',\n",
      "              'G',\n",
      "              'T',\n",
      "              'T',\n",
      "              'C',\n",
      "              'A',\n",
      "              'C',\n",
      "              'T',\n",
      "              'G',\n",
      "              'G',\n",
      "              'G',\n",
      "              'C',\n",
      "              'G',\n",
      "              'T',\n",
      "              'C',\n",
      "              'T',\n",
      "              'T',\n",
      "              'C',\n",
      "              'T',\n",
      "              'G',\n",
      "              'C',\n",
      "              'C',\n",
      "              'C',\n",
      "              'G',\n",
      "              'G',\n",
      "              'C',\n",
      "              'C',\n",
      "              'C',\n",
      "              'C',\n",
      "              'T',\n",
      "              'T',\n",
      "              'C',\n",
      "              'G',\n",
      "              'C',\n",
      "              'C',\n",
      "              'C',\n",
      "              'A',\n",
      "              'C',\n",
      "              'G',\n",
      "              'T',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'C',\n",
      "              'G',\n",
      "              'C',\n",
      "              'C',\n",
      "              'A',\n",
      "              'G',\n",
      "              'G',\n",
      "              'G',\n",
      "              'A',\n",
      "              'G',\n",
      "              'C',\n",
      "              'T',\n",
      "              'G',\n",
      "              'T',\n",
      "              'G',\n",
      "              'A',\n",
      "              'G',\n",
      "              'G',\n",
      "              'C',\n",
      "              'A',\n",
      "              'G',\n",
      "              'T',\n",
      "              'G',\n",
      "              'C',\n",
      "              'T',\n",
      "              'G',\n",
      "              'T',\n",
      "              'G',\n",
      "              'T',\n",
      "              'G',\n",
      "              'G',\n",
      "              'T',\n",
      "              'T',\n",
      "              'C',\n",
      "              'C',\n",
      "              'T',\n",
      "              'G',\n",
      "              'C',\n",
      "              'C',\n",
      "              'G',\n",
      "              'T',\n",
      "              'C',\n",
      "              'C',\n",
      "              'G',\n",
      "              'G',\n",
      "              'A',\n",
      "              'C',\n",
      "              'T',\n",
      "              'C',\n",
      "              'T',\n",
      "              'T',\n",
      "              'T',\n",
      "              'T',\n",
      "              'T',\n",
      "              'C',\n",
      "              'C',\n",
      "              'T',\n",
      "              'C',\n",
      "              'T',\n",
      "              'A',\n",
      "              'C',\n",
      "              'T',\n",
      "              'G',\n",
      "              'A',\n",
      "              'G',\n",
      "              'A',\n",
      "              'T',\n",
      "              'T',\n",
      "              'C',\n",
      "              'A',\n",
      "              'T',\n",
      "              'C',\n",
      "              'T',\n",
      "              'G',\n",
      "              'T',\n",
      "              'G',\n",
      "              'T',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'A',\n",
      "              'T',\n",
      "              'A',\n",
      "              'T',\n",
      "              'G',\n",
      "              'A',\n",
      "              'G',\n",
      "              'T',\n",
      "              'T',\n",
      "              'G',\n",
      "              'G',\n",
      "              'C',\n",
      "              'G',\n",
      "              'A',\n",
      "              'G',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'G',\n",
      "              'A',\n",
      "              'T',\n",
      "              'C',\n",
      "              'G',\n",
      "              'A',\n",
      "              'C',\n",
      "              'C',\n",
      "              'T',\n",
      "              'A',\n",
      "              'T',\n",
      "              'T',\n",
      "              'A',\n",
      "              'T',\n",
      "              'T',\n",
      "              'G',\n",
      "              'G',\n",
      "              'C',\n",
      "              'C',\n",
      "              'T',\n",
      "              'A',\n",
      "              'G',\n",
      "              'A',\n",
      "              'C',\n",
      "              'C',\n",
      "              'A',\n",
      "              'A',\n",
      "              'G',\n",
      "              'G',\n",
      "              'C',\n",
      "              'G',\n",
      "              'C',\n",
      "              'T',\n",
      "              'A',\n",
      "              'T',\n",
      "              'G',\n",
      "              'T',\n",
      "              'A',\n",
      "              'C',\n",
      "              'A',\n",
      "              'G',\n",
      "              'C',\n",
      "              'C',\n",
      "              'T',\n",
      "              'C',\n",
      "              'C',\n",
      "              'T',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'A',\n",
      "              'T',\n",
      "              'G',\n",
      "              'A',\n",
      "              'T',\n",
      "              'T',\n",
      "              'G',\n",
      "              'G',\n",
      "              'G',\n",
      "              'C',\n",
      "              'C',\n",
      "              'T',\n",
      "              'A',\n",
      "              'T',\n",
      "              'G',\n",
      "              'C',\n",
      "              'G',\n",
      "              'G',\n",
      "              'C',\n",
      "              'C',\n",
      "              'C',\n",
      "              'G',\n",
      "              'A',\n",
      "              'G',\n",
      "              'C',\n",
      "              'A',\n",
      "              'G',\n",
      "              'T',\n",
      "              'T',\n",
      "              'C',\n",
      "              'A',\n",
      "              'G',\n",
      "              'T',\n",
      "              'G',\n",
      "              'A',\n",
      "              'T',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'G',\n",
      "              'T',\n",
      "              'G',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'C',\n",
      "              'C',\n",
      "              'A',\n",
      "              'G',\n",
      "              'C',\n",
      "              'A',\n",
      "              'A',\n",
      "              'C',\n",
      "              'A',\n",
      "              'C',\n",
      "              'C',\n",
      "              'T',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'G',\n",
      "              'G',\n",
      "              'G',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'C',\n",
      "              'C',\n",
      "              'A',\n",
      "              'G',\n",
      "              'C',\n",
      "              'A',\n",
      "              'A',\n",
      "              'C',\n",
      "              'T',\n",
      "              'C',\n",
      "              'A',\n",
      "              'A',\n",
      "              'C',\n",
      "              'G',\n",
      "              'T',\n",
      "              'C',\n",
      "              'A',\n",
      "              'G',\n",
      "              'G',\n",
      "              'A',\n",
      "              'T',\n",
      "              'C',\n",
      "              'C',\n",
      "              'T',\n",
      "              'G',\n",
      "              'C',\n",
      "              'A',\n",
      "              'G',\n",
      "              'C',\n",
      "              'T',\n",
      "              'G',\n",
      "              'C',\n",
      "              'T',\n",
      "              'C',\n",
      "              'A',\n",
      "              'G',\n",
      "              'G',\n",
      "              'A',\n",
      "              'G',\n",
      "              'G',\n",
      "              'G',\n",
      "              'A',\n",
      "              'G',\n",
      "              'A',\n",
      "              'G',\n",
      "              'G',\n",
      "              'A',\n",
      "              'T',\n",
      "              'G',\n",
      "              'A',\n",
      "              'G',\n",
      "              'G',\n",
      "              'G',\n",
      "              'A',\n",
      "              'G',\n",
      "              'C',\n",
      "              'A',\n",
      "              'T',\n",
      "              'C',\n",
      "              'T',\n",
      "              'G',\n",
      "              'C',\n",
      "              'A',\n",
      "              'G',\n",
      "              'G',\n",
      "              'T',\n",
      "              'C',\n",
      "              'A',\n",
      "              'A',\n",
      "              'G',\n",
      "              'G',\n",
      "              'G',\n",
      "              'C',\n",
      "              'C',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'G',\n",
      "              'C',\n",
      "              'C',\n",
      "              'T',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'G',\n",
      "              'C',\n",
      "              'T',\n",
      "              'C',\n",
      "              'A',\n",
      "              'T',\n",
      "              'A',\n",
      "              'G',\n",
      "              'C',\n",
      "              'C',\n",
      "              'A',\n",
      "              'G',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'C',\n",
      "              'A',\n",
      "              'G',\n",
      "              'G',\n",
      "              'G',\n",
      "              'T',\n",
      "              'C',\n",
      "              'A',\n",
      "              'C',\n",
      "              'C',\n",
      "              'C',\n",
      "              'A',\n",
      "              'C',\n",
      "              'A',\n",
      "              'G',\n",
      "              'A',\n",
      "              'C',\n",
      "              'T',\n",
      "              'G',\n",
      "              'G',\n",
      "              'G',\n",
      "              'T',\n",
      "              'G',\n",
      "              'T',\n",
      "              'G',\n",
      "              'A',\n",
      "              'G',\n",
      "              'T',\n",
      "              'G',\n",
      "              'T',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'G',\n",
      "              'A',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              'T',\n",
      "              'G',\n",
      "              'G',\n",
      "              'T',\n",
      "              'C',\n",
      "              'C',\n",
      "              'T',\n",
      "              'G',\n",
      "              'A',\n",
      "              'T',\n",
      "              'G',\n",
      "              'G',\n",
      "              'G',\n",
      "              'C',\n",
      "              'A',\n",
      "              'G',\n",
      "              'G',\n",
      "              'A',\n",
      "              'G',\n",
      "              'A',\n",
      "              'T',\n",
      "              'G',\n",
      "              'G',\n",
      "              'A',\n",
      "              'C',\n",
      "              'C',\n",
      "              'C',\n",
      "              'G',\n",
      "              'C',\n",
      "              'C',\n",
      "              'A',\n",
      "              'A',\n",
      "              'A',\n",
      "              'T',\n",
      "              'C',\n",
      "              'C',\n",
      "              'A',\n",
      "              'G',\n",
      "              'A',\n",
      "              'G',\n",
      "              'G',\n",
      "              'A',\n",
      "              'G',\n",
      "              'G',\n",
      "              'T',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'A',\n",
      "              'A',\n",
      "              'C',\n",
      "              'G',\n",
      "              'C',\n",
      "              'C',\n",
      "              'T',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'G',\n",
      "              'G',\n",
      "              'T',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'A',\n",
      "              'A',\n",
      "              'G',\n",
      "              'C',\n",
      "              'A',\n",
      "              'A',\n",
      "              'T',\n",
      "              'C',\n",
      "              'A',\n",
      "              'C',\n",
      "              'A',\n",
      "              'G',\n",
      "              'T',\n",
      "              'G',\n",
      "              'T',\n",
      "              'T',\n",
      "              'A',\n",
      "              'A',\n",
      "              'A',\n",
      "              'A',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'G',\n",
      "              'A',\n",
      "              'C',\n",
      "              'A',\n",
      "              'C',\n",
      "              'G',\n",
      "              'T',\n",
      "              'T',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'A',\n",
      "              'T',\n",
      "              'G',\n",
      "              'A',\n",
      "              'T',\n",
      "              'G',\n",
      "              'C',\n",
      "              'A',\n",
      "              'G',\n",
      "              'G',\n",
      "              'C',\n",
      "              'T',\n",
      "              'G',\n",
      "              'C',\n",
      "              'T',\n",
      "              'C',\n",
      "              'C',\n",
      "              'T',\n",
      "              'A',\n",
      "              'T',\n",
      "              'G',\n",
      "              'T',\n",
      "              'T',\n",
      "              'G',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A',\n",
      "              'A',\n",
      "              'T',\n",
      "              'T',\n",
      "              'T',\n",
      "              'G',\n",
      "              'T',\n",
      "              'T',\n",
      "              'C',\n",
      "              'A',\n",
      "              'T',\n",
      "              'T',\n",
      "              'A',\n",
      "              'A',\n",
      "              'A',\n",
      "              'A',\n",
      "              'T',\n",
      "              'T',\n",
      "              'C',\n",
      "              'T',\n",
      "              'C',\n",
      "              'C',\n",
      "              'C',\n",
      "              'A',\n",
      "              'A',\n",
      "              'T',\n",
      "              'A',\n",
      "              'A',\n",
      "              'A',\n",
      "              'G',\n",
      "              'C',\n",
      "              'T',\n",
      "              'T',\n",
      "              'T',\n",
      "              'A',\n",
      "              'C',\n",
      "              'A',\n",
      "              'G',\n",
      "              'C',\n",
      "              'C',\n",
      "              'T',\n",
      "              'T',\n",
      "              'C',\n",
      "              'T',\n",
      "              'G',\n",
      "              'C',\n",
      "              'A',\n",
      "              'A',\n",
      "              'A',\n",
      "              'G',\n",
      "              'A',\n",
      "              'A']}\n"
     ]
    }
   ],
   "source": [
    "print('train_set.fields:', list(train_set.fields.keys()))\n",
    "print('validation_set.fields:', list(validation_set.fields.keys()))\n",
    "print('test_set.fields:', list(test_set.fields.keys()))\n",
    "print()\n",
    "print('size of training set', len(train_set))\n",
    "print('size of validation set', len(validation_set))\n",
    "print()\n",
    "print('content of first training sample:')\n",
    "pprint(vars(train_set[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the vocabularies\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "SEQ.build_vocab(train_set, max_size=None, vectors=Vectors('wiki.simple.vec', url=url))\n",
    "LABEL.build_vocab(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text fields:\n",
      " size of vocabulary: 7\n",
      " vocabulary's embedding dimension: torch.Size([7, 300])\n",
      " no. times the \"N\" appear in the dataset: 85\n",
      "\n",
      "Label fields:\n",
      " list of vocabulary (int-to-str): ['<unk>', '1', '0', '3', '4', '2', '5']\n",
      " list of vocabulary (str-to-int): {'<unk>': 0, '1': 1, '0': 2, '3': 3, '4': 4, '2': 5, '5': 6}\n"
     ]
    }
   ],
   "source": [
    "print('Text fields:')\n",
    "print(' size of vocabulary:', len(SEQ.vocab))\n",
    "print(\" vocabulary's embedding dimension:\", SEQ.vocab.vectors.size())\n",
    "print(' no. times the \"N\" appear in the dataset:', SEQ.vocab.freqs['N'])\n",
    "\n",
    "print('\\nLabel fields:')\n",
    "#print('keys of LABEL.vocab:', list(LABEL.vocab.__dict__.keys()))\n",
    "print(\" list of vocabulary (int-to-str):\", LABEL.vocab.itos)\n",
    "print(\" list of vocabulary (str-to-int):\", dict(LABEL.vocab.stoi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "# make iterator for splits\n",
    "train_iter, val_iter = data.BucketIterator.splits((train_set, validation_set),\n",
    "                                                     batch_size=128, \n",
    "                                                     device=0 if use_cuda else -1,\n",
    "                                                     sort_key=lambda x: len(x.sequence), #Sorting within the batch\n",
    "                                                     sort_within_batch=False,\n",
    "                                                     repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension of batch's text: torch.Size([11747, 128])\n",
      "first sequence in text: tensor([4, 3, 5,  ..., 1, 1, 1])\n",
      "correct label index: tensor(1)\n",
      "the actual label: 1\n"
     ]
    }
   ],
   "source": [
    "# print batch information\n",
    "batch = next(iter(train_iter))\n",
    "print(\"dimension of batch's text:\", batch.sequence.size())\n",
    "print(\"first sequence in text:\", batch.sequence[:,0])\n",
    "print(\"correct label index:\", batch.label[0])\n",
    "print(\"the actual label:\", LABEL.vocab.itos[get_numpy(batch.label[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (embeddings): Embedding(7, 300)\n",
      "  (rnn_1): LSTM(300, 100)\n",
      "  (l_out): Linear(in_features=200, out_features=7, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# size of embeddings\n",
    "embedding_dim = SEQ.vocab.vectors.size()[1]\n",
    "num_embeddings = SEQ.vocab.vectors.size()[0]\n",
    "num_classes = len(LABEL.vocab.itos)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #learn a new embedding\n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "\n",
    "        # use pretrained embeddings\n",
    "        self.embeddings.weight.data.copy_(SEQ.vocab.vectors)\n",
    "        self.embeddings.weight.detach_()\n",
    "        \n",
    "        \n",
    "        self.rnn_1 = LSTM(input_size=embedding_dim,\n",
    "                         hidden_size=100,\n",
    "                         num_layers=1,\n",
    "                         bidirectional=False)\n",
    "        \n",
    "        self.l_out = Linear(in_features=200,\n",
    "                            out_features=num_classes,\n",
    "                            bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = {}\n",
    "        \n",
    "        # get embeddings\n",
    "        x = self.embeddings(x)\n",
    "        \n",
    "        # rnn returns output and last hidden state\n",
    "        x, hn = self.rnn_1(x)\n",
    "        \n",
    "        # get a fixed sized hidden representation of the entire sequence\n",
    "        out['hidden'] = x = torch.cat((torch.mean(x, dim=0), torch.max(x, dim=0)[0]), dim=1)\n",
    "        \n",
    "        # classify\n",
    "        out['out'] = softmax(self.l_out(x), dim=1)\n",
    "        return out\n",
    "\n",
    "net = Net()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "    print(\"using cuda\")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embeddings.weight': False,\n",
       " 'rnn_1.weight_ih_l0': True,\n",
       " 'rnn_1.weight_hh_l0': True,\n",
       " 'rnn_1.bias_ih_l0': True,\n",
       " 'rnn_1.bias_hh_l0': True,\n",
       " 'l_out.weight': True}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check which params require grad\n",
    "{p[0]: p[1].requires_grad for p in net.named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# we filter the model's parameters such that we can remove the embedding layer, \n",
    "# which does not have requires_grad\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()), lr=0.001)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], ts)\n",
    "    # averaging the one-hot encoded vector\n",
    "    return torch.mean(correct_prediction.float())\n",
    "\n",
    "def construct_sentences(batch):\n",
    "    return [\" \".join([SEQ.vocab.itos[elm] \n",
    "                      for elm in get_numpy(batch.sequence[:,i])])\n",
    "            for i in range(batch.sequence.size()[1])]\n",
    "\n",
    "def get_labels(batch):\n",
    "    return [LABEL.vocab.itos[get_numpy(batch.label[i])] for i in range(len(batch.label))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 25000\n",
    "eval_every = 1000\n",
    "log_every = 500\n",
    "\n",
    "train_loss, train_accs = [], []\n",
    "\n",
    "net.train()\n",
    "for i, batch in enumerate(train_iter):\n",
    "    if i % eval_every == 0:\n",
    "        net.eval()\n",
    "        val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "        val_meta = {'label_idx': [], 'sentences': [], 'labels': []}\n",
    "        for val_batch in val_iter:\n",
    "            output = net(val_batch.sequence)\n",
    "            # batches sizes might vary, which is why we cannot just mean the batch's loss\n",
    "            # we multiply the loss and accuracies with the batch's size,\n",
    "            # to later divide by the total size\n",
    "            val_losses += criterion(output['out'], val_batch.label) * val_batch.batch_size\n",
    "            val_accs += accuracy(output['out'], val_batch.label) * val_batch.batch_size\n",
    "            val_lengths += val_batch.batch_size\n",
    "            \n",
    "            for key, _val in output.items():\n",
    "                if key not in val_meta:\n",
    "                    val_meta[key] = []\n",
    "                val_meta[key].append(get_numpy(_val)) \n",
    "            val_meta['label_idx'].append(get_numpy(val_batch.label))\n",
    "            val_meta['sentences'].append(construct_sentences(val_batch))\n",
    "            val_meta['labels'].append(get_labels(val_batch))\n",
    "        \n",
    "        for key, _val in val_meta.items():\n",
    "            val_meta[key] = np.concatenate(_val)\n",
    "        \n",
    "        # divide by the total accumulated batch sizes\n",
    "        val_losses /= val_lengths\n",
    "        val_accs /= val_lengths\n",
    "        \n",
    "        print(\"### EVAL loss: {:.2f} accs: {:.2f}\".format(get_numpy(val_losses),\n",
    "                                                          get_numpy(val_accs)))\n",
    "        net.eval()\n",
    "\n",
    "        \n",
    "    output = net(batch.text)\n",
    "    batch_loss = criterion(output['out'], batch.label)\n",
    "    \n",
    "    train_loss.append(get_numpy(batch_loss))\n",
    "    train_accs.append(get_numpy(accuracy(output['out'], batch.label)))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % log_every == 0:        \n",
    "        print(\"train, it: {} loss: {:.2f} accs: {:.2f}\".format(i, \n",
    "                                                               np.mean(train_loss), \n",
    "                                                               np.mean(train_accs)))\n",
    "        # reset\n",
    "        train_loss, train_accs = [], []\n",
    "        \n",
    "    \n",
    "    if max_iter < i:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
