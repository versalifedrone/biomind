{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loading \n",
    "import torch\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#model \n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataloaders\n",
    "Source : https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definie the data encoder the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data encoder \n",
    "def sparse_encode(seq, rna_dict):\n",
    "    \"\"\"For a given nucleotide sequence, return a sequence with each nucleotide encoded as a one hot vector\"\"\"\n",
    "    enc_seq = np.zeros([len(seq),len(RNA_DICT)])\n",
    "    for i, nuc in enumerate(seq):\n",
    "        enc_seq[i, rna_dict[nuc]] = 1\n",
    "    return enc_seq\n",
    "\n",
    "def encode_with_padding(seq, max_len, rna_dict):\n",
    "    seq_onehot = np.zeros([max_len, len(RNA_DICT)], dtype=np.uint8)\n",
    "    length = len(seq)\n",
    "    encoded_seq = sparse_encode(seq, rna_dict)\n",
    "    seq_onehot[:length,:] = encoded_seq\n",
    "    seq_onehot[length:,5:6] = 1\n",
    "    return(seq_onehot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and encode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL = \"data/val_filtered.txt\"\n",
    "TRAIN = \"data/train_filtered.txt\"\n",
    "TEST = \"data/test_filtered.txt\"\n",
    "\n",
    "RNA_DICT = {'A': 0, 'T': 1, 'C': 2, 'G': 3, 'N': 4, '-': 5}\n",
    "partition = {'train':[], 'validation':[], 'test':[]} #lists containing ids of the sequences \n",
    "labels = {} # {seq_id : class}\n",
    "sequences = {} # {seq_id : encoded_seq}\n",
    "index = 0\n",
    "max_len = 0\n",
    "\n",
    "with open(VAL, 'r') as fIn : \n",
    "    for line in fIn : \n",
    "        #manage seq id \n",
    "        seq_id = \"id_\"+str(index)\n",
    "        index +=1\n",
    "        #read line in file \n",
    "        seq = line.replace(\" \", \"\").strip().split(\",\")\n",
    "        \n",
    "        #search for max seq len\n",
    "        max_len = len(seq[0]) if len(seq[0])>max_len else max_len\n",
    "        #encode sequence\n",
    "        #encod_seq = sparse_encode(seq[0], RNA_DICT)\n",
    "        #store data\n",
    "        partition['validation'].append(seq_id)\n",
    "        labels[seq_id]=int(seq[1])\n",
    "        sequences[seq_id] = seq[0]\n",
    "\n",
    "\n",
    "with open(VAL, 'r') as fIn : \n",
    "    for line in fIn : \n",
    "        #manage seq id \n",
    "        seq_id = \"id_\"+str(index)\n",
    "        index +=1\n",
    "        #read line in file \n",
    "        seq = line.replace(\" \", \"\").strip().split(\",\")\n",
    "        max_len = len(seq[0]) if len(seq[0])>max_len else max_len\n",
    "        #encode sequence\n",
    "        #encod_seq = sparse_encode(seq[0], RNA_DICT)\n",
    "        #store data\n",
    "        partition['train'].append(seq_id)\n",
    "        labels[seq_id]=int(seq[1])\n",
    "        sequences[seq_id] = seq[0]\n",
    "        \n",
    "        \n",
    "with open(TEST, 'r') as fIn : \n",
    "    for line in fIn : \n",
    "        #manage seq id \n",
    "        seq_id = \"id_\"+str(index)\n",
    "        index +=1\n",
    "        #read line in file \n",
    "        seq = line.replace(\" \", \"\").strip().split(\",\")\n",
    "        max_len = len(seq[0]) if len(seq[0])>max_len else max_len\n",
    "        #encode sequence\n",
    "        #encod_seq = sparse_encode(seq[0], RNA_DICT)\n",
    "        #store data\n",
    "        partition['test'].append(seq_id)\n",
    "        labels[seq_id]=int(seq[1])\n",
    "        sequences[seq_id] = seq[0]\n",
    "\n",
    "for seq_id, seq in sequences.items():\n",
    "    sequences[seq_id] = encode_with_padding(seq, max_len, RNA_DICT)\n",
    "\n",
    "list_IDs = partition['train'] + partition['validation'] + partition['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "  \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
    "  def __init__(self, list_IDs, labels, sequences):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.sequences = sequences\n",
    "        \n",
    "  def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        \"\"\"Generates one sample of data\"\"\"\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = self.sequences[ID]\n",
    "        Y = self.labels[ID]\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and validation Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 12,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "\n",
    "params_valid = {'batch_size': 12,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "max_epochs = 100\n",
    "\n",
    "training_set = Dataset(partition['train'], labels, sequences)\n",
    "training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(partition['validation'], labels, sequences)\n",
    "validation_generator = data.DataLoader(validation_set, **params_valid)\n",
    "\n",
    "test_set = Dataset(partition['test'], labels, sequences)\n",
    "test_generator = data.DataLoader(validation_set, **params)\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (l1): Linear(in_features=110700, out_features=50, bias=False)\n",
      "  (l2): Linear(in_features=50, out_features=6, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define network\n",
    "\n",
    "num_l1 = 50\n",
    "l1_features = max_len*6\n",
    "\n",
    "num_out = 6\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1_in_features = l1_features\n",
    "        self.l1 = Linear(in_features=self.l1_in_features, \n",
    "                            out_features=num_l1,\n",
    "                            bias=False)\n",
    "        self.l2 = Linear(in_features=num_l1, \n",
    "                            out_features=num_out,\n",
    "                            bias=False)\n",
    "    \n",
    "    def forward(self, x): # x.size() = [max_len,6]\n",
    "        x = x.view(-1, self.l1_in_features)\n",
    "        x = relu(self.l1(x))\n",
    "        x=self.l2(x)\n",
    "        return softmax(x, dim=1)\n",
    "\n",
    "\n",
    "net = Net()\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    predictions = torch.max(ys, 1)[1]\n",
    "    correct_prediction = torch.eq(predictions, ts)\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1622, 0.1330, 0.1938, 0.1272, 0.2189, 0.1649],\n",
       "        [0.1600, 0.2085, 0.1580, 0.1555, 0.1175, 0.2005],\n",
       "        [0.1453, 0.1453, 0.2159, 0.1279, 0.1609, 0.2047],\n",
       "        [0.1681, 0.1925, 0.1762, 0.1475, 0.1447, 0.1710],\n",
       "        [0.1864, 0.2082, 0.1741, 0.1663, 0.1322, 0.1327],\n",
       "        [0.1334, 0.1035, 0.1376, 0.1992, 0.1608, 0.2655],\n",
       "        [0.1214, 0.1653, 0.1611, 0.1951, 0.1598, 0.1972],\n",
       "        [0.1621, 0.2343, 0.1297, 0.1554, 0.1691, 0.1493],\n",
       "        [0.1661, 0.1845, 0.2090, 0.1411, 0.1225, 0.1768],\n",
       "        [0.1420, 0.1567, 0.1806, 0.1436, 0.1705, 0.2067],\n",
       "        [0.1275, 0.1852, 0.1595, 0.2009, 0.1417, 0.1852],\n",
       "        [0.1524, 0.1452, 0.1753, 0.1447, 0.1747, 0.2077],\n",
       "        [0.1944, 0.1145, 0.2354, 0.1690, 0.1327, 0.1540],\n",
       "        [0.1570, 0.1990, 0.1682, 0.1740, 0.1222, 0.1796],\n",
       "        [0.1852, 0.2041, 0.1936, 0.1337, 0.1139, 0.1695],\n",
       "        [0.1756, 0.2667, 0.1274, 0.1187, 0.1394, 0.1721],\n",
       "        [0.1267, 0.1718, 0.2487, 0.1241, 0.1681, 0.1606],\n",
       "        [0.1241, 0.2211, 0.1371, 0.1580, 0.1715, 0.1881],\n",
       "        [0.1082, 0.1105, 0.2089, 0.2328, 0.1700, 0.1695],\n",
       "        [0.1620, 0.1471, 0.1765, 0.1498, 0.1519, 0.2126],\n",
       "        [0.1513, 0.1985, 0.1744, 0.1267, 0.1627, 0.1865],\n",
       "        [0.1357, 0.1589, 0.1866, 0.1406, 0.1728, 0.2054],\n",
       "        [0.1235, 0.1610, 0.1804, 0.1935, 0.1707, 0.1708],\n",
       "        [0.1225, 0.1935, 0.1843, 0.1552, 0.1516, 0.1928],\n",
       "        [0.1623, 0.1618, 0.1249, 0.1665, 0.1791, 0.2055],\n",
       "        [0.1454, 0.1843, 0.2265, 0.1213, 0.1851, 0.1374],\n",
       "        [0.1493, 0.1756, 0.1576, 0.1374, 0.1874, 0.1927],\n",
       "        [0.1206, 0.1337, 0.1684, 0.1601, 0.1818, 0.2354],\n",
       "        [0.1445, 0.2152, 0.1518, 0.1395, 0.1646, 0.1843],\n",
       "        [0.1694, 0.1961, 0.1812, 0.1228, 0.1665, 0.1639]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test the forward pass with dummy data\n",
    "\n",
    "def randnorm(size):\n",
    "    return np.random.normal(0, 1, size).astype('float32')\n",
    "\n",
    "x = Variable(torch.from_numpy(randnorm((params[\"batch_size\"], max_len, 6))))\n",
    "print(x.type())\n",
    "net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:53: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 3\n",
    "iter_by_epoch = len(training_generator.dataset.list_IDs)/training_generator.batch_size \n",
    "eval_every = int(iter_by_epoch) #eval one time by epoch \n",
    "train_loss, train_accs = [], []\n",
    "valid_loss, valid_accs = [], []\n",
    "valid_iter = []\n",
    "train_iter = []\n",
    "\n",
    "def get_input_and_label(batch):\n",
    "    inputs, labels = batch[0], batch[1] \n",
    "    # Change type\n",
    "    inputs = inputs.type(torch.float)\n",
    "    labels = labels.type(torch.long)\n",
    "    # Transfer to GPU\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    return(inputs, labels)\n",
    "\n",
    "iter_by_epoch = len(training_generator.dataset.list_IDs)/training_generator.batch_size\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    # Training\n",
    "    for i, data in enumerate(training_generator,0):\n",
    "        if i % eval_every == 0:\n",
    "            valid_iter.append(i + (1+epoch)*iter_by_epoch)\n",
    "            net.eval()\n",
    "            val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "            \n",
    "            for num, data in enumerate(validation_generator,0):\n",
    "                inputs, labels = get_input_and_label(data)\n",
    "                output = net(inputs)\n",
    "                val_losses += criterion(output, labels) * num\n",
    "                val_accs += accuracy(output, labels) * num\n",
    "                val_lengths += num\n",
    "\n",
    "            # divide by the total accumulated batch sizes\n",
    "            val_losses /= val_lengths\n",
    "            val_accs /= val_lengths\n",
    "            valid_loss.append(get_numpy(val_losses))\n",
    "            valid_accs.append(get_numpy(val_accs))\n",
    "            net.train()\n",
    "        inputs, labels = get_input_and_label(data) \n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data[0]\n",
    "\n",
    "        train_loss.append(get_numpy(loss))\n",
    "        train_accs.append(get_numpy(accuracy(outputs, labels)))\n",
    "        train_iter.append(i + (epoch+1)*iter_by_epoch)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_iter, train_loss, label='train_loss')\n",
    "plt.plot(valid_iter, valid_loss, label='valid_loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_iter, train_accs, label='train_accs')\n",
    "plt.plot(valid_iter, valid_accs, label='valid_accs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "100\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "for item in validation_generator:\n",
    "    print(len(item[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
