{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loading \n",
    "import torch\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#model \n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, GRU, Conv2d, Dropout, MaxPool2d, BatchNorm1d\n",
    "from torch.nn.functional import relu, elu, relu6, sigmoid, tanh, softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataloaders\n",
    "Source : https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definie the data encoder the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data encoder \n",
    "def sparse_encode(seq, rna_dict):\n",
    "    \"\"\"For a given nucleotide sequence, return a sequence with each nucleotide encoded as a one hot vector\"\"\"\n",
    "    enc_seq = np.zeros([len(seq),len(RNA_DICT)])\n",
    "    for i, nuc in enumerate(seq):\n",
    "        enc_seq[i, rna_dict[nuc]] = 1\n",
    "    return enc_seq\n",
    "\n",
    "def encode_with_padding(seq, max_len, rna_dict):\n",
    "    seq_onehot = np.zeros([max_len, len(RNA_DICT)], dtype=np.uint8)\n",
    "    length = len(seq)\n",
    "    encoded_seq = sparse_encode(seq, rna_dict)\n",
    "    seq_onehot[:length,:] = encoded_seq\n",
    "    seq_onehot[length:,5:6] = 1\n",
    "    return seq_onehot\n",
    "\n",
    "def dynamic_batch_encode(seqs_batch, rna_dict):\n",
    "    \"\"\"For a batch of sequences, gets the longest sequence and pads all sequences to that length.\"\"\"\n",
    "    max_len = len(max(seqs_batch, key=len))\n",
    "    n_seq = len(seqs_batch)\n",
    "    pad_enc_seqs = np.zeros([n_seq, max_len, len(rna_dict)])\n",
    "    \n",
    "    for i in range(n_seq):\n",
    "        pad_enc_seqs[i, :, :] = encode_with_padding(seqs_batch[i], max_len, RNA_DICT)\n",
    "        \n",
    "    return pad_enc_seqs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and encode the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAL = \"data/val_filtered.txt\"\n",
    "TRAIN = \"data/train_filtered.txt\"\n",
    "TEST = \"data/test_filtered.txt\"\n",
    "\n",
    "RNA_DICT = {'A': 0, 'T': 1, 'C': 2, 'G': 3, 'N': 4, '-': 5}\n",
    "partition = {'train':[], 'validation':[], 'test':[]} #lists containing ids of the sequences \n",
    "labels = {} # {seq_id : class}\n",
    "sequences = {} # {seq_id : encoded_seq}\n",
    "index = 0\n",
    "max_len = 0\n",
    "\n",
    "with open(VAL, 'r') as fIn : \n",
    "    for line in fIn : \n",
    "        #manage seq id \n",
    "        seq_id = \"id_\"+str(index)\n",
    "        index +=1\n",
    "        #read line in file \n",
    "        seq = line.replace(\" \", \"\").strip().split(\",\")\n",
    "        \n",
    "        #search for max seq len\n",
    "        max_len = len(seq[0]) if len(seq[0])>max_len else max_len\n",
    "        #encode sequence\n",
    "        #encod_seq = sparse_encode(seq[0], RNA_DICT)\n",
    "        #store data\n",
    "        partition['validation'].append(seq_id)\n",
    "        labels[seq_id]=int(seq[1])\n",
    "        sequences[seq_id] = seq[0]\n",
    "\n",
    "\n",
    "with open(TRAIN, 'r') as fIn : \n",
    "    for line in fIn : \n",
    "        #manage seq id \n",
    "        seq_id = \"id_\"+str(index)\n",
    "        index +=1\n",
    "        #read line in file \n",
    "        seq = line.replace(\" \", \"\").strip().split(\",\")\n",
    "        max_len = len(seq[0]) if len(seq[0])>max_len else max_len\n",
    "        #encode sequence\n",
    "        #encod_seq = sparse_encode(seq[0], RNA_DICT)\n",
    "        #store data\n",
    "        partition['train'].append(seq_id)\n",
    "        labels[seq_id]=int(seq[1])\n",
    "        sequences[seq_id] = seq[0]\n",
    "        \n",
    "        \n",
    "with open(TEST, 'r') as fIn : \n",
    "    for line in fIn : \n",
    "        #manage seq id \n",
    "        seq_id = \"id_\"+str(index)\n",
    "        index +=1\n",
    "        #read line in file \n",
    "        seq = line.replace(\" \", \"\").strip().split(\",\")\n",
    "        max_len = len(seq[0]) if len(seq[0])>max_len else max_len\n",
    "        #encode sequence\n",
    "        #encod_seq = sparse_encode(seq[0], RNA_DICT)\n",
    "        #store data\n",
    "        partition['test'].append(seq_id)\n",
    "        labels[seq_id]=int(seq[1])\n",
    "        sequences[seq_id] = seq[0]\n",
    "\n",
    "#for seq_id, seq in sequences.items():\n",
    "#    sequences[seq_id] = encode_with_padding(seq, max_len, RNA_DICT)\n",
    "\n",
    "list_IDs = partition['train'] + partition['validation'] + partition['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    \"\"\"Characterizes a dataset for PyTorch\"\"\"\n",
    "    def __init__(self, list_IDs, labels, sequences):\n",
    "        \"\"\"Initialization\"\"\"\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.sequences = sequences\n",
    "           \n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of samples\"\"\"\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generates one sample of data\"\"\"\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = self.sequences[ID]\n",
    "        Y = self.labels[ID]\n",
    "\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and validation Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA for PyTorch\n",
    "#use_cuda = torch.cuda.is_available()\n",
    "use_cuda = False\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 30,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "\n",
    "params_valid = {'batch_size': 30,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}\n",
    "max_epochs = 100\n",
    "\n",
    "training_set = Dataset(partition['train'], labels, sequences)\n",
    "training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(partition['validation'], labels, sequences)\n",
    "validation_generator = data.DataLoader(validation_set, **params_valid)\n",
    "\n",
    "test_set = Dataset(partition['test'], labels, sequences)\n",
    "test_generator = data.DataLoader(validation_set, **params)\n",
    "\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(6, 6), stride=(1, 1))\n",
      "  (globalpool): MaxPool2d(kernel_size=50000, stride=50000, padding=25000, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#functions to compute the dimemensions of the output of conv layers\n",
    "def compute_conv1_dim(dim_size):\n",
    "    return int(((dim_size - kernel_size_conv1 + 2 * padding_conv1) / (stride_conv1) + 1))\n",
    "\n",
    "channels = 1\n",
    "input_len = max_len\n",
    "input_width = 6\n",
    "num_filters_conv1 = 6\n",
    "kernel_size_conv1 = 6 \n",
    "padding_conv1 = 0\n",
    "stride_conv1 = 1\n",
    "\n",
    "num_classes = 6\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=channels,\n",
    "                              out_channels = num_filters_conv1,\n",
    "                              kernel_size = kernel_size_conv1,\n",
    "                              stride=stride_conv1,\n",
    "                              padding=padding_conv1)\n",
    "        \n",
    "        #self.maxpool = nn.MaxPool2d(2, stride = 2)\n",
    "        \n",
    "        self.globalpool = nn.MaxPool2d(50000, 50000, 25000)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.from_numpy(np.expand_dims(x, axis=1))\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.globalpool(x)\n",
    "        x = x.view(30, -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net(num_classes)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    predictions = torch.max(ys, 1)[1]\n",
    "    correct_prediction = torch.eq(predictions, ts)\n",
    "    return torch.mean(correct_prediction.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "torch.Size([30, 8191, 6])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 6])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test the forward pass with dummy data\n",
    "\n",
    "def randnorm(size):\n",
    "    return np.random.normal(0, 1, size).astype('float32')\n",
    "\n",
    "x = Variable(torch.from_numpy(randnorm((params[\"batch_size\"],max_len, 6))))\n",
    "#x = get_variable(x)\n",
    "\n",
    "for i, data in enumerate(training_generator,0):\n",
    "    x = data\n",
    "    break\n",
    "inputs, labels = get_input_and_label(x)\n",
    "print(len(x[0]))\n",
    "print(inputs.shape)\n",
    "net(inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 9648, 6)\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(training_generator,0):\n",
    "    print(dynamic_batch_encode(data[0], RNA_DICT).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 3\n",
    "iter_by_epoch = len(training_generator.dataset.list_IDs)/training_generator.batch_size \n",
    "eval_every = int(iter_by_epoch) #eval one time by epoch \n",
    "train_loss, train_accs = [], []\n",
    "valid_loss, valid_accs = [], []\n",
    "valid_iter = []\n",
    "train_iter = []\n",
    "\n",
    "def get_input_and_label(batch):\n",
    "    inputs, labels = batch[0], batch[1] \n",
    "    # Change type\n",
    "    inputs = dynamic_batch_encode(inputs, RNA_DICT)\n",
    "    inputs = torch.from_numpy(inputs).float()\n",
    "    #inputs = inputs.type(torch.float)\n",
    "    labels = labels.type(torch.long)\n",
    "    # Transfer to GPU\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    return(inputs, labels)\n",
    "\n",
    "iter_by_epoch = len(training_generator.dataset.list_IDs)/training_generator.batch_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "    # Training\n",
    "    for i, data in enumerate(training_generator,0):\n",
    "\n",
    "        if i % eval_every == 0:\n",
    "            valid_iter.append(i + (1+epoch)*iter_by_epoch)\n",
    "            net.eval()\n",
    "            val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "            \n",
    "            for num, data in enumerate(validation_generator,0):\n",
    "                inputs, labels = get_input_and_label(data)\n",
    "                output = net(inputs)\n",
    "                val_losses += criterion(output, labels) * num\n",
    "                val_accs += accuracy(output, labels) * num\n",
    "                val_lengths += num\n",
    "\n",
    "            # divide by the total accumulated batch sizes\n",
    "            val_losses /= val_lengths\n",
    "            val_accs /= val_lengths\n",
    "            valid_loss.append(get_numpy(val_losses))\n",
    "            valid_accs.append(get_numpy(val_accs))\n",
    "            net.train()\n",
    "            \n",
    "        inputs, labels = get_input_and_label(data) \n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.data[0]\n",
    "\n",
    "        train_loss.append(get_numpy(loss))\n",
    "        train_accs.append(get_numpy(accuracy(outputs, labels)))\n",
    "        train_iter.append(i + (epoch+1)*iter_by_epoch)\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Loss')\n",
    "plt.plot(train_iter, train_loss, label='train_loss')\n",
    "plt.plot(valid_iter, valid_loss, label='valid_loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_iter, train_accs, label='train_accs')\n",
    "plt.plot(valid_iter, valid_accs, label='valid_accs')\n",
    "plt.xlabel('Iterations')\n",
    "plt.title('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
