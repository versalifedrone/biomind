{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a5bcb7244b9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchqrnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mQRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQRNNLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\biomind\\torchqrnn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mforget_mult\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mForgetMult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mqrnn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mQRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQRNNLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\biomind\\torchqrnn\\forget_mult.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcupy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpynvrtc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompiler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mProgram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from torchqrnn import QRNN, QRNNLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-242a2a940f93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpprint\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpprint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGloVe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pprint import pprint\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, RNN, LSTM, GRU, Conv1d\n",
    "import torch.nn.functional as F\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_DIRECTORY = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8858011ca454>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0muse_cuda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;34m\"\"\" Converts tensors to cuda, if available. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "def get_variable(x):\n",
    "    \"\"\" Converts tensors to cuda, if available. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def get_numpy(x):\n",
    "    \"\"\" Get numpy array for both cuda and not. \"\"\"\n",
    "    if use_cuda:\n",
    "        return x.cpu().data.numpy()\n",
    "    return x.data.numpy()\n",
    "\n",
    "print(f\"Using CUDA: {use_cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the two fields: Sequence and Class\n",
    "SEQ = data.Field(sequential=True,include_lengths=True, unk_token='N')\n",
    "LABEL = data.Field(sequential=False, unk_token='1') # is_target = True ?\n",
    "\n",
    "train_set, validation_set, test_set = data.TabularDataset.splits(path='',\n",
    "                                                                 train=DATA_DIRECTORY+'train_filtered_new.txt',\n",
    "                                                                 validation=DATA_DIRECTORY+'val_filtered.txt',\n",
    "                                                                 test=DATA_DIRECTORY+'val_filtered.txt', \n",
    "                                                                 format = 'csv',\n",
    "                                                                 fields=[('sequence', SEQ), ('label', LABEL)])\n",
    "\n",
    "# build the vocabularies\n",
    "SEQ.build_vocab(train_set, min_freq=2) #NO unknows \n",
    "LABEL.build_vocab(train_set, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "train_iter = data.BucketIterator(train_set,\n",
    "                                 batch_size=BATCH_SIZE, \n",
    "                                 device=torch.device(\"cuda:0\" if use_cuda else \"cpu\"),\n",
    "                                 sort_key=lambda x: len(x.sequence), #Sorting within the batch\n",
    "                                 sort = True,\n",
    "                                 shuffle = True,\n",
    "                                 repeat = False\n",
    "                                )\n",
    "\n",
    "validation_iter = data.BucketIterator(validation_set,\n",
    "                                 batch_size=BATCH_SIZE, \n",
    "                                 device=torch.device(\"cuda:0\" if use_cuda else \"cpu\"),\n",
    "                                 sort_key=lambda x: len(x.sequence), #Sorting within the batch\n",
    "                                sort = True,\n",
    "                                repeat = False\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of embeddings\n",
    "EMBEDDING_DIM = 8\n",
    "NUM_EMBEDDING = len(SEQ.vocab) #size of vocab \n",
    "NUM_CLASSES = len(LABEL.vocab.itos)\n",
    "CONV_CHANNELS = 64\n",
    "HIDDEN_DIM = 1024\n",
    "NUM_LAYERS_QRNN = 2\n",
    "STRIDE = 3\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #learn a new embedding\n",
    "        self.embeddings = nn.Embedding(NUM_EMBEDDING, EMBEDDING_DIM)\n",
    "\n",
    "        # use pretrained embeddings\n",
    "        self.conv1 = Conv1d(in_channels= EMBEDDING_DIM,\n",
    "                            out_channels= CONV_CHANNELS,\n",
    "                            padding=1,\n",
    "                            kernel_size=3,\n",
    "                            stride=STRIDE)\n",
    "\n",
    "        \n",
    "        #self.conv_batchnorm = torch.nn.BatchNorm1d(5*CONV_CHANNELS, track_running_stats=False, momentum=0.5)\n",
    "        \n",
    "        \n",
    "        #self.lstm = LSTM(input_size=EMBEDDING_DIM,\n",
    "        #                 hidden_size=HIDDEN_DIM,\n",
    "        #                 num_layers=NUM_LAYERS_LSTM,\n",
    "        #                 bidirectional=False)\n",
    "        \n",
    "        #self.qrnn = QRNN(6*CONV_CHANNELS, HIDDEN_DIM, num_layers=NUM_LAYERS_QRNN, window=1, zoneout=0.1)\n",
    "        self.qrnn1 = QRNN(CONV_CHANNELS, HIDDEN_DIM, num_layers=NUM_LAYERS_QRNN, window=1, zoneout=0.1)\n",
    "        self.qrnn2 = QRNN(CONV_CHANNELS, HIDDEN_DIM, num_layers=NUM_LAYERS_QRNN, window=1, zoneout=0.1)\n",
    "        self.qrnn1.cuda()\n",
    "        self.qrnn2.cuda()\n",
    "        \n",
    "        # link hidden to tag \n",
    "        self.linear = Linear(in_features=2*HIDDEN_DIM,\n",
    "                            out_features=NUM_CLASSES)\n",
    "        \n",
    "        self.lin_dropout = torch.nn.Dropout(p=0.3)\n",
    "        \n",
    "        \n",
    "        #init lstm hidden units \n",
    "        self.hidden = self.init_hidden(BATCH_SIZE)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        sequences = x[0]\n",
    "        lengths = x[1]\n",
    "#         out = {}\n",
    "        batch_size = sequences.size()[1]\n",
    "        # get embeddings\n",
    "        x = self.embeddings(sequences)\n",
    "        x = x.permute([1,2,0])\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #c2 = F.relu(self.conv2(x))\n",
    "        #c3 = F.relu(self.conv3(x))\n",
    "        #c4 = F.relu(self.conv4(x))\n",
    "        #c5 = F.relu(self.conv5(x))\n",
    "        #c6 = F.relu(self.conv6(x))\n",
    "        #x = torch.cat((c1,c2,c3,c4,c5,c6), 1)\n",
    "        #x = self.conv_batchnorm(x)\n",
    "        x1 = x.permute([2,0,1])\n",
    "        \n",
    "        x2 = torch.flip(x1,(0,))  \n",
    "        \n",
    "        #embeds = nn.utils.rnn.pack_padded_sequence(embeds,lengths/3, batch_first=False)\n",
    "        # rnn returns output and last hidden state\n",
    "        \n",
    "        _, x1 = self.qrnn1(x1)\n",
    "        _, x2 = self.qrnn2(x2)\n",
    "        \n",
    "        x = torch.cat((x1,x2), 2)\n",
    "        \n",
    "        x = self.lin_dropout(x)\n",
    "        #lstm_out, self.hidden = self.lstm(embeds)\n",
    "        #unpacked, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(lstm_out, batch_first=False)\n",
    "        x = F.relu(x[-1].type(torch.float))\n",
    "        #last_output = self.dropout(last_output)\n",
    "        \n",
    "        freqs = F.softmax(self.linear(x))\n",
    "        return freqs\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Before we've done anything, we dont have any hidden state.\n",
    "        # Refer to the Pytorch documentation to see exactly\n",
    "        # why they have this dimensionality.\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (torch.zeros(NUM_LAYERS_QRNN, BATCH_SIZE, HIDDEN_DIM),\n",
    "                torch.zeros(NUM_LAYERS_QRNN, BATCH_SIZE, HIDDEN_DIM))\n",
    "    \n",
    "    \n",
    "net = Net()\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "net.apply(weights_init)\n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "    net.cuda()\n",
    "    print(\"using cuda\")\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which params require grad\n",
    "print(\"List of gradient-requiring parameters in network\")\n",
    "{p[0]: p[1].requires_grad for p in net.named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weights = [1/5531, 1/3486 ,1/2874, 1/1578, 1/814, 1/328]\n",
    "loss_weights = torch.Tensor([item/sum(loss_weights) for item in loss_weights])\n",
    "criterion = nn.CrossEntropyLoss(weight=loss_weights.cuda()) #weight = final_weights\n",
    "# we filter the model's parameters such that we can remove the embedding layer, \n",
    "# which does not have requires_grad\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.0001)\n",
    "\n",
    "\n",
    "def accuracy(ys, ts):\n",
    "    # making a one-hot encoded vector of correct (1) and incorrect (0) predictions\n",
    "    #print(ys)\n",
    "    #print(ts)\n",
    "    correct_prediction = torch.eq(torch.max(ys, 1)[1], ts)\n",
    "#     print(torch.max(ys, 1)[1], ts)\n",
    "#     print(correct_prediction)\n",
    "    # averaging the one-hot encoded vector\n",
    "    return torch.mean(correct_prediction.float())\n",
    "\n",
    "def construct_sentences(batch):\n",
    "    return [\" \".join([SEQ.vocab.itos[elm] \n",
    "                      for elm in get_numpy(batch.sequence[:,i])])\n",
    "            for i in range(batch.sequence.size()[1])]\n",
    "\n",
    "def get_labels(batch):\n",
    "    return [LABEL.vocab.itos[get_numpy(batch.label[i])] for i in range(len(batch.label))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_allocated() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 60\n",
    "iter_by_epoch = int(len(train_set)/BATCH_SIZE)\n",
    "eval_every = 1000\n",
    "\n",
    "reduced_train_lost = []\n",
    "reduced_train_accuracy = []\n",
    "\n",
    "reduced_val_lost = []\n",
    "reduced_val_accuracy = []\n",
    "\n",
    "times = []\n",
    "epoch_index = []\n",
    "i = 0\n",
    "\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    train_loss, train_accs = [], []\n",
    "    val_loss, val_accs = [], []\n",
    "    \n",
    "    net.train()\n",
    "    # Training\n",
    "    for i, batch in enumerate(train_iter):\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = net(batch.sequence)\n",
    "        batch_loss = criterion(output, batch.label)\n",
    "        \n",
    "        batch_loss.backward()     \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss.append(get_numpy(batch_loss.detach()))\n",
    "        train_accs.append(get_numpy(accuracy(output, batch.label).detach()))\n",
    "\n",
    "        if i % eval_every == 0:     \n",
    "            #print(f\"OUTPUT:{output}\")\n",
    "            \n",
    "            net.eval()\n",
    "            val_losses_running, val_accs_running, val_lengths = 0, 0, 0\n",
    "            \n",
    "            for val_batch in validation_iter:\n",
    "                output = net(val_batch.sequence).detach()\n",
    "                val_losses_running += criterion(output, val_batch.label)*val_batch.batch_size\n",
    "                val_accs_running += accuracy(output, val_batch.label)*val_batch.batch_size\n",
    "                val_lengths += val_batch.batch_size\n",
    "\n",
    "            # divide by the total accumulated batch sizes\n",
    "            val_losses_running /= val_lengths\n",
    "            val_accs_running /= val_lengths\n",
    "            val_loss.append(get_numpy(val_losses_running))\n",
    "            val_accs.append(get_numpy(val_accs_running))\n",
    "            net.train()\n",
    "        \n",
    "    \n",
    "    reduced_train_lost.append(np.mean(train_loss))\n",
    "    reduced_train_accuracy.append(np.mean(train_accs))\n",
    "    \n",
    "    reduced_val_lost.append(np.mean(val_loss))\n",
    "    reduced_val_accuracy.append(np.mean(val_accs))\n",
    "    \n",
    "    epoch_index.append(epoch)\n",
    "    times.append(time.time())\n",
    "    \n",
    "    print(f'\\nEpoch {epoch}\\tTrain {np.mean(train_loss):.3f}\\tVal {np.mean(val_loss):.3f}\\tTAcc {np.mean(train_accs):.3f}\\tVAcc {np.mean(val_accs):.3f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epoch_index,reduced_train_lost,label='train_loss')\n",
    "plt.plot(epoch_index,reduced_val_lost, label='valid_loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epoch_index, reduced_train_accuracy, label='train_accs')\n",
    "plt.plot(epoch_index, reduced_val_accuracy, label='valid_accs')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig.savefig(str(time.time())+\".svg\", format=\"svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "valid_loss = []\n",
    "valid_accs = []\n",
    "\n",
    "val_losses, val_accs, val_lengths = 0, 0, 0\n",
    "predictions = []\n",
    "real_values = []\n",
    "\n",
    "\n",
    "for val_batch in validation_iter:\n",
    "    output = net(val_batch.sequence).detach()\n",
    "    predictions.extend([LABEL.vocab.itos[int(i)] for i in torch.max(output,1)[1]])\n",
    "    real_values += [LABEL.vocab.itos[int(i)] for i in val_batch.label]\n",
    "    val_losses += criterion(output, val_batch.label)*val_batch.batch_size\n",
    "    val_accs += accuracy(output, val_batch.label) *val_batch.batch_size\n",
    "    val_lengths += val_batch.batch_size\n",
    "\n",
    "# divide by the total accumulated batch sizes\n",
    "val_losses /= val_lengths\n",
    "val_accs /= val_lengths\n",
    "valid_loss.append(get_numpy(val_losses))\n",
    "valid_accs.append(get_numpy(val_accs))\n",
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import itertools\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting normalize=True.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "#Stupid modification to get order in confusion matrix\n",
    "cnf_matrix = confusion_matrix(real_values, predictions)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "import seaborn as sns; sns.set()\n",
    "ax = plt.axes()\n",
    "\n",
    "sns.heatmap(cnf_matrix,annot=True, cmap= \"GnBu\",fmt=\"d\", xticklabels = [\"Cytoplasm\", \"Nucleus\", \"Ribosome\", \"Endoplasmatic Reticulum\", \"Secreted\", \"Mitochondria\"], yticklabels = [\"Cytoplasm\", \"Nucleus\", \"Ribosome\", \"Endoplasmatic Reticulum\", \"Secreted\", \"Mitochondria\"])\n",
    "ax.set_ylabel('True label')\n",
    "ax.set_xlabel('Predicted label')\n",
    "# plt.savefig(\"double_lstm_confusion.png\")\n",
    "plt.show()\n",
    "\n",
    "cm = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]\n",
    "print(\"By class accuracies :\\n\")\n",
    "print(cm.diagonal())\n",
    "\n",
    "print(\"total accuracy :\\n\")\n",
    "print(accuracy_score(real_values, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
